{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Simran/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from statsmodels.tools import eval_measures\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on approach and thoughts for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Approach: Linear Regression Model is the simplest form of neural network, so it should be a good baseline model to compare other approaches. From my prior understanding of epidemiology, I don't expect the dengue outbreaks to be modeled linearly. I anticipate it to follow some sort of logistic model. Average temperature, city, year, week of year, and average precipitation are good baseline features to use. City, year, and week of year will provide sufficient information for time and place. As the spread of diseases is highly dependent on the number of already infected people, this information is vital for a successful model. Temperature and precipitation are valuable features as they are useful in predicting the amount of mosquitos in an area. The disease vector (female mosquito) like damp, warm environments so these features should be useful in predicting future dengue outbreaks. For the example, we will have 3 hidden layers with 10, 20, and 10 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model to try to use is the Long Short Term Memory Model. The LSTM model is recurrent, meaning data can flow forward and backward throughout the network making it useful to model situations where there are many time gaps between important events. These time gaps are characteristic of the data we have present for the dengue fever outbreaks. Also, since the model is recurrent, it can make good use of the time stamps we have and are more likely to accurately predict future outbreaks as the spread of disease is heavily dependent on variables such as number of previous cases in an area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the female mosquito population is the disease vector for Dengue fever, you can naturally assume a connection between their population in an area and the prevalence of the disease. Finding the mosquito population in SJ and IQ over a certain period of time can be helpful in increasing the accuracy of the model. This information can be ideally downloaded from the internet. If not, it may be possible to create a feature that can be used to represent the population of mosquitos as a function of the mosquito's preferred environmental conditions such as humidity, temperature, water content, etc. Reducing these factors into one feature can also provide a way for you to control the weight of each individual individual factor. For example, weighting the temperature twice as much as other factors if you think that it is more important in calculating the mosquito count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mosquitos are also daytime feeders so time of day may be important in determining the likelihood of infection. \"The peak biting periods are early in the morning and in the evening before dusk.\" (from WHO) A rough estimate of time of day can be derived from information about the time of year given from the features. This can be used to create a \"daytime\" scale that can be used to represent different amounts of sunlight in a given time period. For example, summer has comparatively longer and hotter days and this may be used to complement temperature data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_labels = pd.read_csv('./input/dengue_labels_train.csv')\n",
    "total_features = pd.read_csv('./input/dengue_features_train.csv')\n",
    "total_test_features = pd.read_csv('./input/dengue_features_test.csv')\n",
    "total_labels = total_labels.drop(total_labels.columns[[0, 1, 2]], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'year', 'weekofyear', 'week_start_date', 'ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw', 'precipitation_amt_mm', 'reanalysis_air_temp_k', 'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k', 'reanalysis_precip_amt_kg_per_m2', 'reanalysis_relative_humidity_percent', 'reanalysis_sat_precip_amt_mm', 'reanalysis_specific_humidity_g_per_kg', 'reanalysis_tdtr_k', 'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c', 'station_min_temp_c', 'station_precip_mm']\n"
     ]
    }
   ],
   "source": [
    "print((total_features.columns.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [0, 1, 2, 3, 4, 5, 6, 8, 7, 9, 10, 12, 14, 15, 16, 18, 19, 20, 21, 23]\n",
    "total_test_features = total_test_features.drop(total_test_features.columns[remove], axis=1) \n",
    "total_features = total_features.drop(total_features.columns[remove], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = total_features.fillna(method='ffill')\n",
    "total_test_features = total_test_features.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reanalysis_dew_point_temp_k', 'reanalysis_min_air_temp_k', 'reanalysis_specific_humidity_g_per_kg', 'station_min_temp_c']\n"
     ]
    }
   ],
   "source": [
    "print((total_features.columns.values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for new features. I experimented in creating my own time scale and creating a \"mosquito count\" feature to model the mosquito population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def normalize(column):\n",
    "#     new_column = []\n",
    "#     min_val = min(column)\n",
    "#     max_val = max(column)\n",
    "#     range_val = max_val-min_val\n",
    "#     for entry in column:\n",
    "#         new_column.append((entry-min_val)/range_val)\n",
    "#     return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_times = []\n",
    "# for entry in total_features['week_start_date']:\n",
    "#     entry = entry.split('/')\n",
    "#     if int(entry[2]) > 89:\n",
    "#         entry[2] = '19' + entry[2]\n",
    "#     else: entry[2] = '20' + entry[2]\n",
    "#     newtime = datetime.datetime(int(entry[2]), int(entry[0]), int(entry[1])).toordinal() - 726587\n",
    "#     new_times.append(newtime)\n",
    "# total_features['new_times'] = new_times\n",
    "    \n",
    "# new_times = []\n",
    "# offset = datetime.datetime.strptime(total_test_features['week_start_date'][0], '%Y-%m-%d').toordinal()\n",
    "# for entry in total_test_features['week_start_date']:\n",
    "#     new_times.append(datetime.datetime.strptime(entry, '%Y-%m-%d').toordinal()-726587)\n",
    "# total_test_features['new_times'] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_features = total_features.drop(total_features.columns[[0, 3]], axis=1) \n",
    "# total_features = total_features.drop(total_features.index[[1456]], axis=0) \n",
    "# total_labels = total_labels.drop(total_labels.columns[[0, 1, 2]], axis=1) \n",
    "# total_test_features = total_test_features.drop(total_test_features.columns[[0, 3]], axis=1)\n",
    "# total_features = total_features.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "# total_test_features = total_test_features.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "# new_feature = total_features['reanalysis_precip_amt_kg_per_m2']+(total_features['reanalysis_avg_temp_k']/25)+(total_features['reanalysis_relative_humidity_percent']/10)\n",
    "# total_features['new'] = new_feature\n",
    "# new_test_feature = total_test_features['reanalysis_precip_amt_kg_per_m2']+(total_test_features['reanalysis_avg_temp_k']/25)+(total_test_features['reanalysis_relative_humidity_percent']/10)\n",
    "# total_test_features['new'] = new_test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_features = total_features.drop(total_features.columns[[0, 1, 2, 3, 4, 6]], axis=1) \n",
    "# total_test_features = total_test_features.drop(total_test_features.columns[remove], axis=1)\n",
    "# total_features = total_features[['reanalysis_specific_humidity_g_per_kg', \n",
    "#                  'reanalysis_dew_point_temp_k', \n",
    "#                  'station_avg_temp_c', \n",
    "#                  'station_min_temp_c']]\n",
    "# total_test_features = total_test_features[['reanalysis_specific_humidity_g_per_kg', \n",
    "#                  'reanalysis_dew_point_temp_k', \n",
    "#                  'station_avg_temp_c', \n",
    "#                  'station_min_temp_c']]\n",
    "# total_features = total_features.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "# total_test_features = total_test_features.apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_to_change = ['reanalysis_precip_amt_kg_per_m2', 'reanalysis_avg_temp_k', 'reanalysis_sat_precip_amt_mm', 'reanalysis_relative_humidity_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for feature in features_to_change:\n",
    "#     new_train = normalize(total_features[feature])\n",
    "#     new_test = normalize(total_test_features[feature])\n",
    "#     total_features['new_'+feature] = new_train\n",
    "#     total_test_features['new_'+feature] = new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_features = total_features.drop(total_features.columns[remove_later], axis=1) \n",
    "# total_test_features = total_test_features.drop(total_test_features.columns[remove_later], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic DNN Regressor model is implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sj_features = total_features.loc[:935][:]\n",
    "iq_features = total_features.loc[936:][:]\n",
    "sj_labels = total_labels.loc[:935][:]\n",
    "iq_labels = total_labels.loc[936:][:]\n",
    "sj_test_features = total_test_features[:260][:]\n",
    "iq_test_features = total_test_features[261:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sj_x_train, sj_x_test, sj_y_train, sj_y_test = model_selection.train_test_split(sj_features, sj_labels, \n",
    "                                                                    test_size=0.2, random_state=42)\n",
    "iq_x_train, iq_x_test, iq_y_train, iq_y_test = model_selection.train_test_split(iq_features, iq_labels, \n",
    "                                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    }
   ],
   "source": [
    "sj_feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(sj_x_train)\n",
    "iq_feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(iq_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11f462a20>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpd5_06z6f\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11f462ba8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmp160nu8id\n",
      "WARNING:tensorflow:From <ipython-input-19-b339c6dc41a4>:5: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-19-b339c6dc41a4>:5: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpd5_06z6f/model.ckpt.\n",
      "INFO:tensorflow:loss = 5911.11, step = 1\n",
      "INFO:tensorflow:global_step/sec: 273.879\n",
      "INFO:tensorflow:loss = 2638.19, step = 101 (0.367 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpd5_06z6f/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2636.57.\n",
      "WARNING:tensorflow:From <ipython-input-19-b339c6dc41a4>:6: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-19-b339c6dc41a4>:6: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmp160nu8id/model.ckpt.\n",
      "INFO:tensorflow:loss = 688.764, step = 1\n",
      "INFO:tensorflow:global_step/sec: 393.453\n",
      "INFO:tensorflow:loss = 121.653, step = 101 (0.256 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmp160nu8id/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 121.599.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:335: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:733: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpd5_06z6f/model.ckpt-200\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b339c6dc41a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msj_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msj_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msj_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msj_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: {0:f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0miq_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "sj_regressor = tf.contrib.learn.DNNRegressor(feature_columns=sj_feature_columns, \n",
    "                                            hidden_units=[10, 20])\n",
    "iq_regressor = tf.contrib.learn.DNNRegressor(feature_columns=iq_feature_columns, \n",
    "                                            hidden_units=[10, 20])\n",
    "sj_regressor.fit(sj_x_train, sj_y_train, steps=200)\n",
    "iq_regressor.fit(iq_x_train, iq_y_train, steps=200)\n",
    "predictions = list(sj_regressor.predict(sj_x_test, as_iterable=True))\n",
    "sj_score = metrics.mean_absolute_error(sj_y_test, predictions)\n",
    "print('Loss: {0:f}'.format(score))\n",
    "predictions = list(iq_regressor.predict(iq_x_test, as_iterable=True))\n",
    "iq_score = metrics.mean_absolute_error(iq_y_test, predictions)\n",
    "print('Loss: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 29.081833\n",
      "Loss: 6.237430\n"
     ]
    }
   ],
   "source": [
    "print('Loss: {0:f}'.format(sj_score))\n",
    "print('Loss: {0:f}'.format(iq_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:335: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:733: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpos1g2vr7/model.ckpt-200\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:335: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /Users/Simran/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:733: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/3d/9yysmf897wj96_hjgtfwnjnw0000gn/T/tmpos1g2vr7/model.ckpt-200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-578c8033002c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                          index_col=[0, 1, 2])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msj_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miq_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2997\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2999\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3000\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2496\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "sj_predictions = [int(round(i)) for i in list(sj_regressor.predict(sj_test_features))]\n",
    "iq_predictions = [int(round(i)) for i in list(sj_regressor.predict(iq_test_features))]\n",
    "\n",
    "submission = pd.read_csv('./input/submission_format.csv',\n",
    "                         index_col=[0, 1, 2])\n",
    "\n",
    "submission.total_cases = np.concatenate([sj_predictions, iq_predictions])\n",
    "submission.to_csv('./input/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layer Perceptron Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardprop(X, w_1, w_2):\n",
    "    h = tf.nn.sigmoid(tf.matmul(X, w_1))  # The \\sigma function\n",
    "    yhat = tf.matmul(h, w_2)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(features, labels):\n",
    "    return train_test_split(features, labels, test_size=0.33, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_train_X, sj_test_X, sj_train_y, sj_test_y = process_features(sj_features, sj_labels)\n",
    "iq_train_X, iq_test_X, iq_train_y, iq_test_y = process_features(iq_features, iq_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer's sizes\n",
    "x_size = sj_train_X.shape[1]\n",
    "h_size = 512\n",
    "y_size = sj_train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbols\n",
    "X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, y_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight initializations\n",
    "w_1 = init_weights((x_size, h_size))\n",
    "w_2 = init_weights((h_size, y_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "yhat    = forwardprop(X, w_1, w_2)\n",
    "predict = tf.to_int64(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "cost    = tf.losses.mean_squared_error(labels=y, predictions=yhat)\n",
    "updates = tf.train.GradientDescentOptimizer(0.0000003).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train loss = 29.06, test loss = 30.94\n",
      "Epoch = 2, train loss = 27.09, test loss = 28.98\n",
      "Epoch = 3, train loss = 26.07, test loss = 27.98\n",
      "Epoch = 4, train loss = 24.94, test loss = 26.85\n",
      "Epoch = 5, train loss = 24.46, test loss = 26.35\n",
      "Epoch = 6, train loss = 24.29, test loss = 26.16\n",
      "Epoch = 7, train loss = 24.10, test loss = 25.97\n",
      "Epoch = 8, train loss = 24.10, test loss = 25.95\n",
      "Epoch = 9, train loss = 24.12, test loss = 25.97\n"
     ]
    }
   ],
   "source": [
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(9):\n",
    "        # Train with each example\n",
    "        for i in range(len(sj_train_X)):\n",
    "            sess.run(updates, feed_dict={X: sj_train_X[i: i + 1], y: sj_train_y[i: i + 1]})\n",
    "            \n",
    "        train_accuracy = metrics.mean_absolute_error(sj_train_y, sess.run(predict, feed_dict={X: sj_train_X}))\n",
    "        test_accuracy  = metrics.mean_absolute_error(sj_test_y, sess.run(predict, feed_dict={X: sj_test_X}))\n",
    "\n",
    "        print(\"Epoch = %d, train loss = %.2f, test loss = %.2f\"\n",
    "              % (epoch + 1, train_accuracy, test_accuracy))\n",
    "        \n",
    "mlp_sj_predictions = sess.run(predict, feed_dict={X: sj_test_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train loss = 7.02, test loss = 6.76\n",
      "Epoch = 2, train loss = 6.56, test loss = 6.34\n",
      "Epoch = 3, train loss = 6.56, test loss = 6.34\n",
      "Epoch = 4, train loss = 6.29, test loss = 6.06\n",
      "Epoch = 5, train loss = 6.29, test loss = 6.06\n",
      "Epoch = 6, train loss = 6.29, test loss = 6.06\n",
      "Epoch = 7, train loss = 6.15, test loss = 5.89\n",
      "Epoch = 8, train loss = 6.16, test loss = 5.92\n",
      "Epoch = 9, train loss = 6.16, test loss = 5.92\n",
      "Epoch = 10, train loss = 6.16, test loss = 5.92\n",
      "Epoch = 11, train loss = 6.16, test loss = 5.92\n",
      "Epoch = 12, train loss = 6.10, test loss = 5.94\n",
      "Epoch = 13, train loss = 6.10, test loss = 5.94\n",
      "Epoch = 14, train loss = 6.10, test loss = 5.94\n",
      "Epoch = 15, train loss = 6.10, test loss = 5.94\n"
     ]
    }
   ],
   "source": [
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for epoch in range(15):\n",
    "        # Train with each example\n",
    "        for i in range(len(iq_train_X)):\n",
    "            sess.run(updates, feed_dict={X: iq_train_X[i: i + 1], y: iq_train_y[i: i + 1]})\n",
    "            \n",
    "        train_accuracy = metrics.mean_absolute_error(iq_train_y, sess.run(predict, feed_dict={X: iq_train_X}))\n",
    "        test_accuracy  = metrics.mean_absolute_error(iq_test_y, sess.run(predict, feed_dict={X: iq_test_X}))\n",
    "\n",
    "        print(\"Epoch = %d, train loss = %.2f, test loss = %.2f\"\n",
    "              % (epoch + 1, train_accuracy, test_accuracy))\n",
    "        \n",
    "mlp_iq_predictions = sess.run(predict, feed_dict={X: iq_test_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-82adfa4d1ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                          index_col=[0, 1, 2])\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmlp_sj_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_iq_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/mlp_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2997\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2999\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3000\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2496\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./input/submission_format.csv',\n",
    "                         index_col=[0, 1, 2])\n",
    "\n",
    "submission.total_cases = np.concatenate([mlp_sj_predictions, mlp_iq_predictions])\n",
    "submission.to_csv('./input/mlp_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
