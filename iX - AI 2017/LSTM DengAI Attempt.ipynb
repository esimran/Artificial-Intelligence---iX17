{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(column):\n",
    "    new_column = []\n",
    "    min_val = min(column)\n",
    "    max_val = max(column)\n",
    "    range_val = max_val-min_val\n",
    "    for entry in column:\n",
    "        new_column.append((entry-min_val)/range_val)\n",
    "    return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_labels = pd.read_csv('./input/dengue_labels_train.csv', engine='python')\n",
    "total_test_features = pd.read_csv('./input/dengue_features_test.csv')\n",
    "total_labels = np.ravel(total_labels.drop(total_labels.columns[[0, 1, 2]], axis=1).values.astype('float32'))\n",
    "sj_final_test = total_labels[:260]\n",
    "iq_final_test = total_labels[261:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Simran/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Simran/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Simran/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/Simran/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = sj_labels\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit_transform(dataset)\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "sj_train, sj_test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "\n",
    "dataset = iq_labels\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit_transform(dataset)\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "iq_train, iq_test = dataset[0:train_size], dataset[train_size:len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "sj_trainX, sj_trainY = create_dataset(sj_train, look_back)\n",
    "sj_testX, sj_testY = create_dataset(sj_test, look_back)\n",
    "iq_trainX, iq_trainY = create_dataset(iq_train, look_back)\n",
    "iq_testX, iq_testY = create_dataset(iq_test, look_back)\n",
    "sj_x, sj_y = create_dataset(sj_final_test, look_back)\n",
    "iq_x, iq_y = create_dataset(iq_final_test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "sj_trainX = numpy.reshape(sj_trainX, (sj_trainX.shape[0], 1, sj_trainX.shape[1]))\n",
    "sj_testX = numpy.reshape(sj_testX, (sj_testX.shape[0], 1, sj_testX.shape[1]))\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "iq_trainX = numpy.reshape(iq_trainX, (iq_trainX.shape[0], 1, iq_trainX.shape[1]))\n",
    "iq_testX = numpy.reshape(iq_testX, (iq_testX.shape[0], 1, iq_testX.shape[1]))\n",
    "\n",
    "sj_x = numpy.reshape(sj_x, (sj_x.shape[0], 1, sj_x.shape[1]))\n",
    "iq_x = numpy.reshape(iq_x, (iq_x.shape[0], 1, iq_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5.    4.    3.    6.    2.    4.    5.   10.    6.    8.    2.    6.\n",
      "   17.   23.   13.   21.   28.   24.   20.   40.   27.   42.   33.   43.\n",
      "   37.   57.   71.   44.   56.   53.   52.   47.   26.   27.   21.   21.\n",
      "   26.   34.   37.   17.   19.   25.   18.   21.   17.   17.   16.   16.\n",
      "   15.   23.   16.   17.   12.   17.   10.   15.   19.   21.   14.   18.\n",
      "   13.   14.   18.   23.   25.   62.   60.   76.   66.   64.   68.   89.\n",
      "   92.  140.  116.  142.  129.  140.  140.  127.  129.  169.  141.  108.\n",
      "   78.   70.   81.  104.   90.   85.   55.   53.   65.   33.   38.   59.\n",
      "   40.   37.   29.   30.   30.   28.   23.   24.   29.   26.   23.   20.\n",
      "   19.   20.   26.   29.   31.   28.   26.   32.   35.   33.   30.   52.\n",
      "   59.   67.   65.   74.   70.   61.   53.   76.   61.   57.   44.   34.\n",
      "   47.   60.   60.   53.   36.   31.   30.   32.   28.   33.   33.   35.\n",
      "   22.   13.   13.   21.   17.   11.    8.    8.    6.    6.    7.   12.\n",
      "   17.   10.   10.   18.   19.   12.   22.   12.   21.   18.   16.   16.\n",
      "   22.   17.   25.   23.   12.   25.   28.   27.   18.   23.   23.   29.\n",
      "   38.   36.   43.   46.   31.   25.   40.   31.   38.   30.   22.   31.\n",
      "   26.   35.   36.   39.   25.   31.   37.   33.   25.   24.   18.   23.\n",
      "   13.   18.   14.   17.   22.   13.   24.   31.   34.   31.   31.   38.\n",
      "   49.   42.   49.   55.   80.   84.   72.   89.  115.  179.  202.  272.\n",
      "  302.  395.  426.  461.  381.  333.  353.  410.  364.  359.  288.  221.\n",
      "  149.  112.  154.   91.   72.   56.   46.   37.   26.   17.   17.   20.\n",
      "   11.    7.   16.   14.   16.    5.    2.    6.    5.    4.    3.    4.\n",
      "   16.    8.    7.   10.   14.    7.    9.   11.   23.   17.   19.   24.\n",
      "   17.   28.   40.   33.   31.   33.   29.   30.   36.   48.   40.   28.\n",
      "   36.   19.   34.   23.   17.   17.   23.   14.   20.   13.   23.   20.\n",
      "   16.   16.   23.   14.   15.    4.    5.    5.   11.   11.    7.    4.\n",
      "    6.    5.    2.    4.    2.    4.    6.    6.    4.    6.   11.   16.\n",
      "    9.   12.   13.   27.   21.   19.   17.   24.   27.   30.   29.   25.\n",
      "   35.   33.   30.   29.   31.   29.   22.   27.   24.   26.   29.   22.\n",
      "   33.   24.   30.   20.   17.   24.   28.   18.   13.    9.   14.   11.\n",
      "   11.   19.   10.    8.    8.    9.    3.    7.   14.    4.    9.   14.\n",
      "    7.    9.    3.    3.   14.   12.   10.   21.   26.   47.   42.   31.\n",
      "   34.   33.   52.   56.   70.  112.   70.   47.   48.   49.   66.   56.\n",
      "   61.   67.   64.   68.   49.   50.   56.   75.   63.   62.   41.   50.\n",
      "   34.   31.   38.   30.   32.   26.   30.   36.   35.   46.   48.   44.\n",
      "   51.   59.   71.  102.  128.  127.  150.  191.  256.  329.  263.  220.\n",
      "  204.  181.   99.   54.   80.  102.  127.   73.   68.   64.   55.   67.\n",
      "   84.   85.   67.   73.   89.   68.   59.   56.   77.   75.   47.   50.\n",
      "   42.   28.   37.   37.   27.   12.   15.   22.    8.   15.   17.   10.\n",
      "    9.   11.   20.   13.   11.   16.   11.    7.   17.   14.   13.   15.\n",
      "   30.   25.   40.   44.   25.   21.   48.   56.   60.   45.   55.   32.\n",
      "   46.   61.   42.   37.   43.   34.   40.   25.   16.   17.   17.   16.\n",
      "   23.   18.   18.    9.    7.    7.    4.    3.    2.    8.    3.    1.\n",
      "    1.    2.    3.    3.    2.    0.    0.    2.    2.    0.    6.    3.\n",
      "    6.    2.    3.    2.    4.    5.    2.    9.    2.    4.    8.    6.\n",
      "    3.   11.   14.   15.   20.    9.   20.   28.   38.   30.   30.   23.\n",
      "   16.   22.   28.   14.   17.   20.   17.   10.   13.   20.    9.   18.\n",
      "    9.    8.   19.   11.    4.    6.    6.    8.   13.    8.    8.    5.\n",
      "   16.   12.   11.   18.   10.   22.   14.   16.   18.   27.   38.   35.\n",
      "   41.   51.   65.   55.   54.   62.   64.   56.   65.   71.   75.   71.\n",
      "   72.   47.   27.   35.   25.   19.   37.   38.   34.   26.   19.   18.\n",
      "   22.   16.   18.    6.   12.    6.    6.    3.    7.    6.    1.    3.]\n"
     ]
    }
   ],
   "source": [
    "print(sj_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5s - loss: 5086.3319\n",
      "Epoch 2/100\n",
      "4s - loss: 4888.6072\n",
      "Epoch 3/100\n",
      "4s - loss: 4736.6310\n",
      "Epoch 4/100\n",
      "4s - loss: 4639.1163\n",
      "Epoch 5/100\n",
      "4s - loss: 4553.5422\n",
      "Epoch 6/100\n",
      "5s - loss: 4474.0251\n",
      "Epoch 7/100\n",
      "4s - loss: 4398.5189\n",
      "Epoch 8/100\n",
      "4s - loss: 4327.1965\n",
      "Epoch 9/100\n",
      "4s - loss: 4259.5929\n",
      "Epoch 10/100\n",
      "4s - loss: 4194.9315\n",
      "Epoch 11/100\n",
      "4s - loss: 4133.2580\n",
      "Epoch 12/100\n",
      "4s - loss: 4074.5755\n",
      "Epoch 13/100\n",
      "4s - loss: 4019.4687\n",
      "Epoch 14/100\n",
      "4s - loss: 3967.8469\n",
      "Epoch 15/100\n",
      "4s - loss: 3916.7937\n",
      "Epoch 16/100\n",
      "4s - loss: 3868.1681\n",
      "Epoch 17/100\n",
      "4s - loss: 3821.2586\n",
      "Epoch 18/100\n",
      "4s - loss: 3775.9085\n",
      "Epoch 19/100\n",
      "4s - loss: 3732.9784\n",
      "Epoch 20/100\n",
      "4s - loss: 3692.4698\n",
      "Epoch 21/100\n",
      "4s - loss: 3652.6625\n",
      "Epoch 22/100\n",
      "4s - loss: 3613.5952\n",
      "Epoch 23/100\n",
      "4s - loss: 3575.8650\n",
      "Epoch 24/100\n",
      "4s - loss: 3539.9864\n",
      "Epoch 25/100\n",
      "4s - loss: 3504.5799\n",
      "Epoch 26/100\n",
      "4s - loss: 3471.0620\n",
      "Epoch 27/100\n",
      "4s - loss: 3438.5150\n",
      "Epoch 28/100\n",
      "4s - loss: 3407.2355\n",
      "Epoch 29/100\n",
      "4s - loss: 3376.2545\n",
      "Epoch 30/100\n",
      "4s - loss: 3346.4274\n",
      "Epoch 31/100\n",
      "4s - loss: 3318.1929\n",
      "Epoch 32/100\n",
      "4s - loss: 3289.9299\n",
      "Epoch 33/100\n",
      "4s - loss: 3261.2747\n",
      "Epoch 34/100\n",
      "4s - loss: 3234.3749\n",
      "Epoch 35/100\n",
      "4s - loss: 3207.6368\n",
      "Epoch 36/100\n",
      "4s - loss: 3180.4664\n",
      "Epoch 37/100\n",
      "4s - loss: 3155.8853\n",
      "Epoch 38/100\n",
      "4s - loss: 3131.1116\n",
      "Epoch 39/100\n",
      "4s - loss: 3106.7367\n",
      "Epoch 40/100\n",
      "4s - loss: 3083.3489\n",
      "Epoch 41/100\n",
      "4s - loss: 3060.9644\n",
      "Epoch 42/100\n",
      "4s - loss: 3037.3746\n",
      "Epoch 43/100\n",
      "4s - loss: 3014.9819\n",
      "Epoch 44/100\n",
      "4s - loss: 2992.7549\n",
      "Epoch 45/100\n",
      "4s - loss: 2971.7947\n",
      "Epoch 46/100\n",
      "5s - loss: 2950.3832\n",
      "Epoch 47/100\n",
      "5s - loss: 2929.3601\n",
      "Epoch 48/100\n",
      "4s - loss: 2908.9314\n",
      "Epoch 49/100\n",
      "4s - loss: 2888.9766\n",
      "Epoch 50/100\n",
      "4s - loss: 2869.1256\n",
      "Epoch 51/100\n",
      "4s - loss: 2851.0701\n",
      "Epoch 52/100\n",
      "4s - loss: 2831.7939\n",
      "Epoch 53/100\n",
      "4s - loss: 2813.4843\n",
      "Epoch 54/100\n",
      "4s - loss: 2795.5413\n",
      "Epoch 55/100\n",
      "4s - loss: 2778.3883\n",
      "Epoch 56/100\n",
      "4s - loss: 2760.3273\n",
      "Epoch 57/100\n",
      "4s - loss: 2743.6472\n",
      "Epoch 58/100\n",
      "4s - loss: 2726.9587\n",
      "Epoch 59/100\n",
      "4s - loss: 2709.8200\n",
      "Epoch 60/100\n",
      "4s - loss: 2693.7525\n",
      "Epoch 61/100\n",
      "4s - loss: 2678.7270\n",
      "Epoch 62/100\n",
      "4s - loss: 2662.8910\n",
      "Epoch 63/100\n",
      "4s - loss: 2648.5214\n",
      "Epoch 64/100\n",
      "4s - loss: 2631.8878\n",
      "Epoch 65/100\n",
      "4s - loss: 2618.5118\n",
      "Epoch 66/100\n",
      "4s - loss: 2603.3199\n",
      "Epoch 67/100\n",
      "4s - loss: 2588.6537\n",
      "Epoch 68/100\n",
      "4s - loss: 2573.2389\n",
      "Epoch 69/100\n",
      "4s - loss: 2560.8229\n",
      "Epoch 70/100\n",
      "4s - loss: 2547.9218\n",
      "Epoch 71/100\n",
      "4s - loss: 2533.7152\n",
      "Epoch 72/100\n",
      "4s - loss: 2520.7250\n",
      "Epoch 73/100\n",
      "4s - loss: 2506.4428\n",
      "Epoch 74/100\n",
      "4s - loss: 2493.2966\n",
      "Epoch 75/100\n",
      "4s - loss: 2480.9541\n",
      "Epoch 76/100\n",
      "4s - loss: 2469.2877\n",
      "Epoch 77/100\n",
      "56s - loss: 2457.4230\n",
      "Epoch 78/100\n",
      "4s - loss: 2444.0189\n",
      "Epoch 79/100\n",
      "5s - loss: 2432.4191\n",
      "Epoch 80/100\n",
      "4s - loss: 2419.6556\n",
      "Epoch 81/100\n",
      "4s - loss: 2407.0613\n",
      "Epoch 82/100\n",
      "4s - loss: 2395.9914\n",
      "Epoch 83/100\n",
      "4s - loss: 2383.2553\n",
      "Epoch 84/100\n",
      "4s - loss: 2373.1810\n",
      "Epoch 85/100\n",
      "4s - loss: 2361.5159\n",
      "Epoch 86/100\n",
      "4s - loss: 2351.4087\n",
      "Epoch 87/100\n",
      "4s - loss: 2339.1787\n",
      "Epoch 88/100\n",
      "4s - loss: 2327.8865\n",
      "Epoch 89/100\n",
      "4s - loss: 2317.5568\n",
      "Epoch 90/100\n",
      "4s - loss: 2304.8776\n",
      "Epoch 91/100\n",
      "4s - loss: 2295.6676\n",
      "Epoch 92/100\n",
      "4s - loss: 2285.5870\n",
      "Epoch 93/100\n",
      "4s - loss: 2273.3245\n",
      "Epoch 94/100\n",
      "4s - loss: 2263.7196\n",
      "Epoch 95/100\n",
      "4s - loss: 2251.9243\n",
      "Epoch 96/100\n",
      "4s - loss: 2244.3018\n",
      "Epoch 97/100\n",
      "4s - loss: 2233.3923\n",
      "Epoch 98/100\n",
      "4s - loss: 2220.4786\n",
      "Epoch 99/100\n",
      "4s - loss: 2210.7035\n",
      "Epoch 100/100\n",
      "4s - loss: 2204.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f821d30>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(sj_trainX, sj_trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 7230.12 RMSE\n",
      "Test Score: 2788.00 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(sj_trainX)\n",
    "testPredict = model.predict(sj_testX)\n",
    "sj_predictions = [int(i) for i in model.predict(sj_x)]\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([sj_trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([sj_testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3s - loss: 146.3292\n",
      "Epoch 2/100\n",
      "2s - loss: 132.1529\n",
      "Epoch 3/100\n",
      "2s - loss: 119.0504\n",
      "Epoch 4/100\n",
      "2s - loss: 111.8936\n",
      "Epoch 5/100\n",
      "2s - loss: 107.9576\n",
      "Epoch 6/100\n",
      "2s - loss: 104.0583\n",
      "Epoch 7/100\n",
      "2s - loss: 98.3277\n",
      "Epoch 8/100\n",
      "2s - loss: 95.5136\n",
      "Epoch 9/100\n",
      "2s - loss: 93.5305\n",
      "Epoch 10/100\n",
      "2s - loss: 91.8790\n",
      "Epoch 11/100\n",
      "2s - loss: 90.3886\n",
      "Epoch 12/100\n",
      "2s - loss: 89.0408\n",
      "Epoch 13/100\n",
      "2s - loss: 87.7940\n",
      "Epoch 14/100\n",
      "2s - loss: 86.6599\n",
      "Epoch 15/100\n",
      "2s - loss: 85.6179\n",
      "Epoch 16/100\n",
      "2s - loss: 84.6891\n",
      "Epoch 17/100\n",
      "2s - loss: 83.7484\n",
      "Epoch 18/100\n",
      "2s - loss: 82.9185\n",
      "Epoch 19/100\n",
      "2s - loss: 82.1246\n",
      "Epoch 20/100\n",
      "2s - loss: 81.4252\n",
      "Epoch 21/100\n",
      "2s - loss: 80.6598\n",
      "Epoch 22/100\n",
      "2s - loss: 79.9635\n",
      "Epoch 23/100\n",
      "2s - loss: 79.4307\n",
      "Epoch 24/100\n",
      "2s - loss: 78.7654\n",
      "Epoch 25/100\n",
      "2s - loss: 78.1697\n",
      "Epoch 26/100\n",
      "2s - loss: 77.6217\n",
      "Epoch 27/100\n",
      "2s - loss: 77.0622\n",
      "Epoch 28/100\n",
      "2s - loss: 76.5583\n",
      "Epoch 29/100\n",
      "2s - loss: 76.0560\n",
      "Epoch 30/100\n",
      "2s - loss: 75.6292\n",
      "Epoch 31/100\n",
      "2s - loss: 75.1266\n",
      "Epoch 32/100\n",
      "2s - loss: 74.6635\n",
      "Epoch 33/100\n",
      "2s - loss: 74.2672\n",
      "Epoch 34/100\n",
      "2s - loss: 73.8331\n",
      "Epoch 35/100\n",
      "2s - loss: 73.4431\n",
      "Epoch 36/100\n",
      "2s - loss: 72.9327\n",
      "Epoch 37/100\n",
      "2s - loss: 72.7473\n",
      "Epoch 38/100\n",
      "2s - loss: 72.2318\n",
      "Epoch 39/100\n",
      "2s - loss: 72.0688\n",
      "Epoch 40/100\n",
      "2s - loss: 71.5957\n",
      "Epoch 41/100\n",
      "2s - loss: 71.2680\n",
      "Epoch 42/100\n",
      "2s - loss: 71.0754\n",
      "Epoch 43/100\n",
      "2s - loss: 70.6857\n",
      "Epoch 44/100\n",
      "2s - loss: 70.4779\n",
      "Epoch 45/100\n",
      "2s - loss: 70.1531\n",
      "Epoch 46/100\n",
      "2s - loss: 69.8576\n",
      "Epoch 47/100\n",
      "2s - loss: 69.6004\n",
      "Epoch 48/100\n",
      "2s - loss: 69.4161\n",
      "Epoch 49/100\n",
      "2s - loss: 69.1469\n",
      "Epoch 50/100\n",
      "2s - loss: 69.0145\n",
      "Epoch 51/100\n",
      "2s - loss: 68.7468\n",
      "Epoch 52/100\n",
      "2s - loss: 68.4720\n",
      "Epoch 53/100\n",
      "2s - loss: 68.3302\n",
      "Epoch 54/100\n",
      "2s - loss: 68.0401\n",
      "Epoch 55/100\n",
      "2s - loss: 67.9194\n",
      "Epoch 56/100\n",
      "2s - loss: 67.7414\n",
      "Epoch 57/100\n",
      "2s - loss: 67.6334\n",
      "Epoch 58/100\n",
      "2s - loss: 67.6968\n",
      "Epoch 59/100\n",
      "2s - loss: 67.1929\n",
      "Epoch 60/100\n",
      "2s - loss: 67.0364\n",
      "Epoch 61/100\n",
      "2s - loss: 66.9707\n",
      "Epoch 62/100\n",
      "2s - loss: 66.5667\n",
      "Epoch 63/100\n",
      "2s - loss: 66.6465\n",
      "Epoch 64/100\n",
      "2s - loss: 66.4113\n",
      "Epoch 65/100\n",
      "2s - loss: 66.3549\n",
      "Epoch 66/100\n",
      "2s - loss: 66.3582\n",
      "Epoch 67/100\n",
      "2s - loss: 65.8131\n",
      "Epoch 68/100\n",
      "2s - loss: 66.1127\n",
      "Epoch 69/100\n",
      "2s - loss: 65.7565\n",
      "Epoch 70/100\n",
      "2s - loss: 65.5713\n",
      "Epoch 71/100\n",
      "2s - loss: 65.6519\n",
      "Epoch 72/100\n",
      "2s - loss: 65.5790\n",
      "Epoch 73/100\n",
      "2s - loss: 65.2582\n",
      "Epoch 74/100\n",
      "2s - loss: 65.1681\n",
      "Epoch 75/100\n",
      "2s - loss: 65.3652\n",
      "Epoch 76/100\n",
      "2s - loss: 65.0097\n",
      "Epoch 77/100\n",
      "2s - loss: 64.8981\n",
      "Epoch 78/100\n",
      "2s - loss: 64.7910\n",
      "Epoch 79/100\n",
      "2s - loss: 64.9097\n",
      "Epoch 80/100\n",
      "2s - loss: 64.5288\n",
      "Epoch 81/100\n",
      "2s - loss: 64.9160\n",
      "Epoch 82/100\n",
      "2s - loss: 64.5776\n",
      "Epoch 83/100\n",
      "2s - loss: 64.3401\n",
      "Epoch 84/100\n",
      "2s - loss: 64.4759\n",
      "Epoch 85/100\n",
      "2s - loss: 64.4013\n",
      "Epoch 86/100\n",
      "2s - loss: 64.3447\n",
      "Epoch 87/100\n",
      "2s - loss: 64.2002\n",
      "Epoch 88/100\n",
      "2s - loss: 63.7244\n",
      "Epoch 89/100\n",
      "2s - loss: 64.1085\n",
      "Epoch 90/100\n",
      "2s - loss: 63.9864\n",
      "Epoch 91/100\n",
      "2s - loss: 63.9849\n",
      "Epoch 92/100\n",
      "2s - loss: 63.6484\n",
      "Epoch 93/100\n",
      "2s - loss: 63.8388\n",
      "Epoch 94/100\n",
      "2s - loss: 63.6724\n",
      "Epoch 95/100\n",
      "2s - loss: 63.5236\n",
      "Epoch 96/100\n",
      "2s - loss: 63.8935\n",
      "Epoch 97/100\n",
      "2s - loss: 63.6737\n",
      "Epoch 98/100\n",
      "2s - loss: 63.5266\n",
      "Epoch 99/100\n",
      "2s - loss: 63.6033\n",
      "Epoch 100/100\n",
      "2s - loss: 63.6055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120f20cf8>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(iq_trainX, iq_trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 919.51 RMSE\n",
      "Test Score: 912.47 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(iq_trainX)\n",
    "testPredict = model.predict(iq_testX)\n",
    "iq_predictions = [int(i) for i in model.predict(iq_x)]\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([iq_trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([iq_testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[  16.]]\n",
      "\n",
      " [[  23.]]\n",
      "\n",
      " [[  12.]]\n",
      "\n",
      " [[  14.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  22.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  17.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[  22.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[  21.]]\n",
      "\n",
      " [[  16.]]\n",
      "\n",
      " [[  31.]]\n",
      "\n",
      " [[  25.]]\n",
      "\n",
      " [[  28.]]\n",
      "\n",
      " [[  26.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[  27.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[  38.]]\n",
      "\n",
      " [[  29.]]\n",
      "\n",
      " [[  21.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[  12.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  12.]]\n",
      "\n",
      " [[  19.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[  16.]]\n",
      "\n",
      " [[  21.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[  22.]]\n",
      "\n",
      " [[  37.]]\n",
      "\n",
      " [[  33.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[  83.]]\n",
      "\n",
      " [[ 116.]]\n",
      "\n",
      " [[  32.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[  14.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[  16.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[  14.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[  11.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[  25.]]\n",
      "\n",
      " [[  21.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[  28.]]\n",
      "\n",
      " [[  39.]]\n",
      "\n",
      " [[  20.]]\n",
      "\n",
      " [[  24.]]\n",
      "\n",
      " [[  28.]]\n",
      "\n",
      " [[  26.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[  12.]]\n",
      "\n",
      " [[  18.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   8.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   3.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   7.]]\n",
      "\n",
      " [[   4.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[  13.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[   2.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[   1.]]\n",
      "\n",
      " [[   0.]]\n",
      "\n",
      " [[  14.]]\n",
      "\n",
      " [[   6.]]\n",
      "\n",
      " [[  10.]]\n",
      "\n",
      " [[   5.]]\n",
      "\n",
      " [[  12.]]\n",
      "\n",
      " [[   9.]]\n",
      "\n",
      " [[   5.]]]\n"
     ]
    }
   ],
   "source": [
    "print(iq_trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  6.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  8.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  8.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  8.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[ 23.]]\n",
      "\n",
      " [[ 28.]]\n",
      "\n",
      " [[ 26.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[ 29.]]\n",
      "\n",
      " [[ 58.]]\n",
      "\n",
      " [[ 26.]]\n",
      "\n",
      " [[ 38.]]\n",
      "\n",
      " [[ 35.]]\n",
      "\n",
      " [[ 37.]]\n",
      "\n",
      " [[ 20.]]\n",
      "\n",
      " [[ 29.]]\n",
      "\n",
      " [[ 25.]]\n",
      "\n",
      " [[ 23.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[ 17.]]\n",
      "\n",
      " [[ 19.]]\n",
      "\n",
      " [[ 25.]]\n",
      "\n",
      " [[ 45.]]\n",
      "\n",
      " [[ 34.]]\n",
      "\n",
      " [[ 63.]]\n",
      "\n",
      " [[ 44.]]\n",
      "\n",
      " [[ 50.]]\n",
      "\n",
      " [[ 35.]]\n",
      "\n",
      " [[ 16.]]\n",
      "\n",
      " [[ 16.]]\n",
      "\n",
      " [[ 13.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[ 15.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[ 10.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[ 29.]]\n",
      "\n",
      " [[ 35.]]\n",
      "\n",
      " [[ 30.]]\n",
      "\n",
      " [[ 20.]]\n",
      "\n",
      " [[ 21.]]\n",
      "\n",
      " [[ 12.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  3.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  1.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[  0.]]\n",
      "\n",
      " [[ 10.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[ 17.]]\n",
      "\n",
      " [[ 16.]]\n",
      "\n",
      " [[ 11.]]\n",
      "\n",
      " [[ 12.]]\n",
      "\n",
      " [[ 19.]]\n",
      "\n",
      " [[ 15.]]\n",
      "\n",
      " [[ 12.]]\n",
      "\n",
      " [[ 12.]]\n",
      "\n",
      " [[ 16.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  9.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  8.]]\n",
      "\n",
      " [[  4.]]\n",
      "\n",
      " [[  2.]]\n",
      "\n",
      " [[  7.]]\n",
      "\n",
      " [[  6.]]\n",
      "\n",
      " [[  5.]]\n",
      "\n",
      " [[  8.]]\n",
      "\n",
      " [[  1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(iq_testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518\n"
     ]
    }
   ],
   "source": [
    "print((len(iq_x.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518\n"
     ]
    }
   ],
   "source": [
    "print(len(iq_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-334-220eba26cafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                          index_col=[0, 1, 2])\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msj_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miq_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/lstm_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2997\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2999\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3000\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2496\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simran/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./input/submission_format.csv',\n",
    "                         index_col=[0, 1, 2])\n",
    "\n",
    "submission.total_cases = np.concatenate([sj_predictions, iq_predictions])\n",
    "submission.to_csv('./input/lstm_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
